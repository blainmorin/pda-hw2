---
title: "PHP 2550 - HW#2"
author: "Blain Morin"
date: "October 12, 2018"
output: pdf_document
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}

### Load required packages

require(knitr) || install.packages("knitr")
require(kableExtra) || install.packages("kableExtra")
require(readr) || install.packages("readr")
require(stargazer) || install.packages("stargazer")
require(ggplot2) || install.packages("ggplot2")
require(GGally) || install.packages("GGally")
require(scales) || install.packages("scales")
require(HH) || install.packages("HH")
require(dplyr) || install.package("dplyr")
require(grid) || install.packages("grid")
require(gridExtra) || install.packages("gridExtra")
require(extrafont) || install.packages("extrafont")
require(extrafontdb) || install.packages("extrafontdb")
require(MASS) || install.packages("MASS")

### Add "Computer Modern Font"
font_install('fontcm')


```

# Part A

```{r, echo = FALSE, message = FALSE, warning = FALSE}

### Read in data 
baseline = read_csv("baseseg.csv")

### Select relavent Columns (As per directions)
baseline = baseline %>%
  dplyr::select(gfr, bascre, sbase, dbase, baseu, AGE, SEX, black) %>%
  filter(complete.cases(.))


```

Before model fitting, we first did some exploratory analysis of the data. Here is a summary table of the variables relavant to the assignment (observations with missing values were dropped from analysis): 

```{r, results='asis', echo = FALSE}

### Summary stats for data
stargazer(as.data.frame(baseline), 
          header = FALSE, 
          digits = 2,
          title = "Summary Statistics")

```

We see that the data set contains 1,249 complete observations. The first six rows have the summary statistics for the continuous variables (in order: glomerular filtration rate, baseline creatine, systolic blood pressure, diastolic blood pressure, urine protein, and age). The last two rows are binary variables (sex = 1 if male, black = 1 if black). For the binary variables, the mean represents the proportion of observations with that characteristic. 

We then looked at scatterplots between each of the variable:

```{r, echo = FALSE, warning=FALSE, fig.width=12, fig.height=12}

my_fn <- function(data, mapping, ...){
  p <- ggplot(data = data, mapping = mapping) + 
    geom_point() + 
    geom_smooth(method=loess, fill="red", color="red", se = FALSE) +
    geom_smooth(method=lm, fill="blue", color="blue", se = FALSE) +
    scale_y_continuous(breaks = function(x) unique(floor(pretty(seq(0, (max(x) + 1) * 1.1)))))+
    scale_x_continuous(breaks = function(x) unique(floor(pretty(seq(0, (max(x) + 1) * 1.1)))))+
    theme_classic()
  p
}


gpairs_lower <- function(g){
  g$plots <- g$plots[-(1:g$nrow)]
  g$yAxisLabels <- g$yAxisLabels[-1]
  g$nrow <- g$nrow -1

  g$plots <- g$plots[-(seq(g$ncol, length(g$plots), by = g$ncol))]
  g$xAxisLabels <- g$xAxisLabels[-g$ncol]
  g$ncol <- g$ncol - 1

  g
}

g = baseline %>%
  ggpairs(lower = list(continuous = my_fn),
          upper  = list(continuous = "blank"),
          diag  = list(continuous = "blankDiag")) +
  ggtitle("Pairs Plot") +
  theme(text=element_text(size=10,  family="CM Sans"))

gpairs_lower(g)

```

In the pairs plot above, the blue line is a linear model fit and the red line is a LOESS smooth fit. The first column of plots contains the relationship of our dependent variable (GFR) with all the independent variables in our data set. If the blue line has a positive slope, this indicates a positive relationship between GFR and the independent varbiable. For example, the positive slope in the GFR and sex plot may indicate the males have a higher GFR than women on average. 

We see from the pairs plot that the relationship between GFR and baseline creatine and the relationship between GFR and baseline urine protien may not be linear. We also see in the baseline urine protien plot that the errors in the linear regression are higher for lower GFR values and they are for higher values. These heterogeneous erros would violate our linear model assumptions. We will try to fix these problems using transformations.

Another potential problem with our data is seen in the GFR and black plot. We see that there are observations for a larger range of GFRs for white people, whereas there are only observations of black people with low GFRs. This may represent a sampling bias, which would affect the validity of our counclusions. 

We then explored transformations of GFR, baseline creatine, and baseline urine protein using ladder plots (which create scatterplots of all the combinations of common transformations):

```{r, echo = FALSE}

pdf(family="CM Sans")

trellis.par.set(grid.pars = list(fontfamily = "CM Sans")) 

```

```{r, echo = FALSE}

ladder(gfr ~ bascre, data = baseline,
       main.in = "Ladder of Powers: GFR and Creatine")

ladder(gfr ~ baseu, data = baseline, 
       main.in = "Ladder of Powers: GFR and Urine Protein")


```

In the ladder plots above, we are searching for the combinations that make a linear trend and create a good spread. From visual inspection it, it looks like using a log transformation for GFR, creatine and urine protein may be a good transformation candidate.

We checked these transformations on quantile-quantile plots:

```{r, echo = FALSE}

qqgfr = baseline %>% 
  ggplot(aes(sample = log(gfr))) +
  stat_qq() +
  stat_qq_line() + 
  ggtitle("QQ Plot: log(GFR)") +
  xlab("Theoretical Score") +
  ylab("Sample Score") +
  theme_classic() +
  theme(text=element_text(size=10,  family="CM Sans"))
  

qqbascre = baseline %>% 
  ggplot(aes(sample = I(log(bascre)))) +
  stat_qq() +
  stat_qq_line() + 
  ggtitle("log(QQ Plot: Base Creatine)") +
  xlab("Theoretical Score") +
  ylab("Sample Score") +
  theme_classic() +
  theme(text=element_text(size=10,  family="CM Sans"))

qqbaseu = baseline %>% 
  ggplot(aes(sample = I(log(baseu)))) +
  stat_qq() +
  stat_qq_line() + 
  ggtitle("QQ Plot: log(Base Urine Protein)") +
  xlab("Theoretical Score") +
  ylab("Sample Score") +
  theme_classic() +
  theme(text=element_text(size=10,  family="CM Sans"))

qq.label = textGrob("Transformation Choices", gp=gpar(fontfamily = "CM Sans"))

grid.arrange(qq.label, qqgfr, qqbascre, qqbaseu)

```

In the QQ plots above, the diagonal line represents a perfect normal distribution. We see that the distribution of log(GFR) is a bit left skewed. For log(creatine), the sample quantile is less spread out than a normal distribution, which indicates that the sample data has thin tails. In the urine protein plot, we see that many of the points in the lower quantiles have the same value. This horizontal portion may represent a measurement threshold cutoff. 

We then ran three linear models. We did not include diastolic blood pressure in any of the models because of its high correlation with systolic blood pressure. In the first model we regressed an untransformed GFR on all the other independent variables (except diastolic). In the second model, we used transformed variables as identified above. The third model is uses transformed variables, but drops urine protein from the regression (because of the lack of normality seen in its QQ plot). Here are the results from the three regressions:

```{r, results='asis', echo = FALSE}

attach(baseline)

lm1 = lm(gfr ~ bascre + baseu + sbase + AGE + as.factor(SEX) + as.factor(black))
lm2 = lm(log(gfr) ~ log(bascre) + log(baseu) + sbase + AGE + as.factor(SEX) + as.factor(black))
lm3 = lm(log(gfr) ~ log(bascre) + sbase + AGE + as.factor(SEX) + as.factor(black))

lm1$AIC = AIC(lm1)
lm2$AIC = AIC(lm2)
lm3$AIC = AIC(lm3)

detach(baseline)
stargazer(lm1, lm2, lm3, keep.stat = c("aic", "rsq", "n"), header = FALSE, se = NULL, notes = "Standard errors in ()")

```

We see from the improvement in r squared that model 2 and 3, which used transformed variables, is a better fit than model 1, which does not use transformed variables. Since the AIC is lower in model 2, we know that including log(urine protein) significantly improves our model fit, despite the possible measurement errors found above. 

Next, we checked for variable interactions. From intuition, we decided to graphically check for interactions between race and creatine, urine protein, and blood pressure and also interactions between sex and creatine, urine protein, and blood pressure:

```{r, echo = FALSE, cache = TRUE}

# Black Interactions

### black and basecre
black.cre = baseline %>%
  ggplot(aes(x = I(log(bascre)), y = log(gfr))) + 
  geom_point(aes(color = as.factor(black)), size = .5) +
  geom_smooth(method = "lm", se = FALSE, aes(color = as.factor(black))) +
  theme_classic() +
  theme(text=element_text(size=10,  family="CM Sans"))

### black and baseu
black.eu = baseline %>%
  ggplot(aes(x = I(log(baseu)), y = log(gfr))) + 
  geom_point(aes(color = as.factor(black)), size = .5, show.legend = FALSE) +
  geom_smooth(method = "lm", se = FALSE, aes(color = as.factor(black)), show.legend = FALSE) +
  theme_classic() +
  theme(text=element_text(size=10,  family="CM Sans"))


### black and sbase
black.sbase = baseline %>%
  ggplot(aes(x = sbase, y = log(gfr))) + 
  geom_point(aes(color = as.factor(black)), size = .5, show.legend = FALSE) +
  geom_smooth(method = "lm", se = FALSE, aes(color = as.factor(black)), show.legend = FALSE) +
  theme_classic() +
  theme(text=element_text(size=10,  family="CM Sans"))

black.label = textGrob("Black Interactions", gp=gpar(fontfamily = "CM Sans"))

grid.arrange(black.label, black.cre, black.eu, black.sbase)


```

```{r, echo = FALSE}

# Male interactions

### male and basecre
male.cre = baseline %>%
  ggplot(aes(x = I(log(bascre)), y = log(gfr))) + 
  geom_point(aes(color = as.factor(SEX)), size = .5) +
  geom_smooth(method = "lm", se = FALSE, aes(color = as.factor(SEX))) +
  ggtitle("Baseline Creatine") +
  xlab("GFR") +
  ylab("Baseline Creatine ^-.5") +
  labs(color = "Sex") +
  theme_classic() +
  theme(text=element_text(size=10,  family="CM Sans"))

### black and baseu
male.eu = baseline %>%
  ggplot(aes(x = I(log(baseu)), y = log(gfr))) + 
  geom_point(aes(color = as.factor(SEX)), size = .5, show.legend = FALSE) +
  geom_smooth(method = "lm", se = FALSE, aes(color = as.factor(SEX)), show.legend = FALSE) +
  ggtitle("Basline Urine Protein") +
  xlab("log(Baseline Urine Protein)") +
  ylab("GFR") +
  theme_classic() +
  theme(text=element_text(size=10,  family="CM Sans"))


### black and sbase
male.sbase = baseline %>%
  ggplot(aes(x = sbase, y = log(gfr))) + 
  geom_point(aes(color = as.factor(SEX)), size = .5, show.legend = FALSE) +
  geom_smooth(method = "lm", se = FALSE, aes(color = as.factor(SEX)), show.legend = FALSE) +
  ggtitle("Baseline Systolic BP") +
  xlab("Systolic BP") +
  ylab("GFR") +
  theme_classic() +
  theme(text=element_text(size=10,  family="CM Sans"))

male.label = textGrob("Male Interactions", gp=gpar(fontfamily = "CM Sans"))

grid.arrange(male.label, male.cre, male.eu, male.sbase)

```

In the scatter plots above, a significant interaction would surface as a difference in slopes between the two groups. Visually, it appears that there may be a significant interaction between race and blood pressure and also a significant interaction between sex and blood pressure. To check this significance, we added the two interactions to model (2) from above:

```{r, results = 'asis', echo = FALSE}

attach(baseline)

lm4 = lm(log(gfr) ~ I(bascre^(-.5)) + 
           log(baseu) + 
           sbase + AGE + 
           as.factor(SEX) + as.factor(black) +
           as.factor(black):sbase +
           as.factor(SEX):sbase)

lm4$AIC = AIC(lm4)

stargazer(lm4, keep.stat = c("aic", "rsq", "n"), header = FALSE)

detach(baseline)

```


We see that the interaction terms are not significant. 

We can also brute force check every combination of second order interactions:

```{r, results = 'asis', echo = FALSE}

lm5 = lm(log(gfr) ~ (log(bascre) + log(baseu) + sbase + AGE + as.factor(SEX) + as.factor(black))^2, 
         data = baseline)

anova.table = as.data.frame(anova(lm5))

anova.table = anova.table %>%
  rename(p = "Pr(>F)")

anova.table %>%
  kable(format = "latex", booktabs = TRUE, caption = "All Second Order Interactions") %>%
  row_spec(row = which(anova.table$p <.05), color = "red")

```

```{r, echo = FALSE, results = 'asis'}

lm6 = lm(log(gfr) ~ log(bascre) + 
           log(baseu) + sbase + AGE +
           as.factor(SEX) + as.factor(black) + 
           log(bascre):log(baseu), data = baseline)

lm6$AIC = AIC(lm6)

stargazer(lm6, keep.stat = c("aic", "rsq", "n"), header = FALSE)

```
# Part B

## 1. First you will explore type 1 and type 2 errors with a simple regression.

### a. Simulate a random vector y of length 100 in which each element follows a normal distribution with mean 10 and variance 4. Then simulate a random vector x of length 100 with mean 3 and variance 1.

```{r}

set.seed(10)

y = rnorm(n =100, mean = 10, sd = 2)
x = rnorm(n = 100, mean = 3, sd = 1)


```

### b. Regress y on x and save the p-value for the slope (HINT: use the summary.lm function or summary(lmobject) and use the coef element of the summary object if using R)

```{r}

reg1 = lm(y ~ x)

reg1.sum = summary.lm(reg1)

reg1.p = reg1.sum$coefficients[2,4]

```

### c. What do you think this p-value should be?

### d. Now replicate this procedure 1000 times to develop a simulation.

```{r}

set.seed(88)

n = 100
n.sims = 1000

xs = matrix(0, nrow = n, ncol = n.sims)
ys = matrix(0, nrow = n, ncol = n.sims)

for (i in 1:n.sims) {
  
  xs[,i] = rnorm(n = 100, mean = 3, sd = 1)
  
}

for (i in 1:n.sims) {
  
  ys[,i] = rnorm(n =100, mean = 10, sd = 2)
  
}



ps = c()

for (i in 1:n.sims) {
  
  reg = lm(ys[,i] ~ xs[,i])
  p = summary.lm(reg)
  reg.p = p$coefficients[2,4]
  ps[i] = reg.p
  
}



```

###  e. Calculate the proportion of times the p-value is less than 0.05. How does this match with your intuition? [HINT: Should the slope be related to the outcome? What type of error are you simulating?]

```{r}

length(which(ps < .05)) / n.sims


```

### f. 

Now simulate y and x together so that the conditional mean of y given x is 10+x and the conditional variance is 1. Repeat b-e above for this distribution. How does your answer differ from the first simulation? What are you simulating now? [HINT: Consider if the slope is actually related to the outcome]

```{r}


set.seed(99)

xs2 = matrix(0, nrow = n, ncol = n.sims)
ys2 = matrix(0, nrow = n, ncol = n.sims)

for (i in 1:n.sims) {
  
  xs2[,i] = rnorm(n = 100, mean = 3, sd = 1)
  
}

for (j in 1:n.sims) {
  
  for (i in 1:n) {
    
    ys2[i,j] = rnorm(n = 1, mean = xs2[i,j] + 10, sd = 1)
    
  }
  
  
}


ps2 = c()
for (i in 1:n.sims) {
  
  reg = lm(ys2[,i] ~ xs2[,i])
  p = summary.lm(reg)
  reg.p = p$coefficients[2,4]
  ps2[i] = reg.p
    
  
}


length(which(ps2 < .05)) / n.sims


```


## 2. Now we will investigate overfitting which occurs when you are making too complex a model. First we will simulate some data in which y and multiple x's are unrelated.

### a. Extend the function you have written to carry out exercise 1 so that you now generate the same y vector as in 1a (i.e. 100 draws from N(10,4)) but you now generate 100 draws from an 3-variate multivariate normal X matrix with means 1, 2, 3 and covariance matrix equal to the identity matrix of rank 3 (i.e., each X variable has variance 1 and they are uncorrelated). [HiNT: use the mvrnorm function in the MASS library to generate multivariate normal draws].

### b. Regress y on the X matrix and save the p-values from this regression. In R if you specify X as a matrix in the call to the lm() function, it will automatically use the formula y~x[,1]+x[,2]+â€¦ i.e. it will use the columns of the matrix as the predictors.

```{r}

set.seed(100)
params = 3
sigma = diag(1, nrow = 3, ncol = 3)


mv.ps = matrix(0, nrow = n.sims, ncol = params)

for (i in 1:n.sims) {
  
  x = mvrnorm(n = 100, mu = c(1,2,3), Sigma = sigma)
  y = rnorm(n =100, mean = 10, sd = 2)
  reg = lm(y ~ x)
  sum.reg = summary.lm(reg)
  reg.p = sum.reg$coefficients[I(1:params+1), 4]
  mv.ps[i,] = reg.p
  
}

```

### c. Determine how many of the p-values for each variable are significant at the 0.05 level.

```{r}

apply(mv.ps, 2, function(x) length(which(x < .05)))

```


### d. Compute the minimum p-value for each regression and calculate how many times the minimum p-value is less than 0.05. Why is this answer different from those in question c?

```{r}

min.p = c()

for (i in 1:n.sims) {
  
  min.p[i] = min(mv.ps[i,])
  
}


length(which(min.p < .05))


```

### e. Now repeat this exercise changing the number of predictors P so that you do it for P = 3, 5, 10, 20, 50, 90. Compute the minimum p-value in each simulated regression and plot the number of times you find at least one significant variable for each P. What pattern do you see? How do you explain this?

```{r}

set.seed(4)

params = c(3, 5, 10, 20, 50, 90)

results = matrix(0, nrow = length(params), ncol = 2)

for (i in 1:length(params)){
  
  sigma = diag(1, nrow = params[i], ncol = params[i])
  results[i, 1] = params[i]
  mv.ps = matrix(0, nrow = n.sims, ncol = params[i])
  
  for (j in 1:n.sims) {
    
    x = mvrnorm(n = 100, mu = 1:params[i], Sigma = sigma)
    y = rnorm(n =100, mean = 10, sd = 2)
    reg = lm(y ~ x)
    sum.reg = summary.lm(reg)
    reg.p = sum.reg$coefficients[I(1:params[i]+1), 4]
    mv.ps[j,] = reg.p
    
  }
  
  min.p = apply(mv.ps, 1, min)
  results[i, 2] = length(which(min.p < .05))
  
  
  
}


```

### e. Set P equal to every number from 1 to 99 (i.e. you try models with all possible number of predictors) and run more simulations to reduce simulation error.

```{r, cache=TRUE}

set.seed(4)

params = 1:99

results2 = matrix(0, nrow = length(params), ncol = 2)

for (i in 1:length(params)){
  
  sigma = diag(1, nrow = params[i], ncol = params[i])
  results2[i, 1] = params[i]
  mv.ps = matrix(0, nrow = n.sims, ncol = params[i])
  
  for (j in 1:n.sims) {
    
    x = mvrnorm(n = 100, mu = 1:params[i], Sigma = sigma)
    y = rnorm(n =100, mean = 10, sd = 2)
    reg = lm(y ~ x)
    sum.reg = summary.lm(reg)
    reg.p = sum.reg$coefficients[I(1:params[i]+1), 4]
    mv.ps[j,] = reg.p
    
  }
  
  min.p = apply(mv.ps, 1, min)
  results2[i, 2] = length(which(min.p < .05))
  
  
  
}


```


